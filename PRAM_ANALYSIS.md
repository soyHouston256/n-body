# ğŸ§® ANÃLISIS PRAM Y COMPLEJIDAD TEÃ“RICA
## SimulaciÃ³n N-Body Gravitacional

---

## ğŸ”„ **MODELO PRAM (Parallel Random Access Machine)**

### **Algoritmo N-Body Hermite Paralelo**

```
PRAM N-Body Algorithm:
Input: N partÃ­culas, P procesadores
Output: EvoluciÃ³n temporal del sistema

FASE 1: INICIALIZACIÃ“N
for i = 0 to P-1 in parallel:
    local_particles[i] = N/P partÃ­culas
    broadcast(global_state)
end parallel

FASE 2: BUCLE TEMPORAL PRINCIPAL
while t < t_end:
    
    SUBFASE 2.1: PREDICCIÃ“N (Paralela)
    for i = 0 to P-1 in parallel:
        for j in local_particles[i]:
            predict_position(j, dt[j])    // O(1) por partÃ­cula
            predict_velocity(j, dt[j])    // O(1) por partÃ­cula
        end for
    end parallel
    
    SUBFASE 2.2: COMUNICACIÃ“N (SincronizaciÃ³n)
    all_gather(predicted_positions)       // O(log P) con Ã¡rbol
    
    SUBFASE 2.3: CÃLCULO DE FUERZAS (Paralela)
    for i = 0 to P-1 in parallel:
        for j in local_particles[i]:
            force[j] = 0
            for k = 0 to N-1:            // O(N) interacciones
                if k â‰  j:
                    force[j] += gravity(j,k)  // O(1) por par
                end if
            end for
        end for
    end parallel
    
    SUBFASE 2.4: CORRECCIÃ“N HERMITE (Paralela)
    for i = 0 to P-1 in parallel:
        for j in local_particles[i]:
            correct_position(j)           // O(1) por partÃ­cula
            correct_velocity(j)           // O(1) por partÃ­cula
            update_timestep(j)            // O(1) por partÃ­cula
        end for
    end parallel
    
    SUBFASE 2.5: REDUCCIÃ“N TEMPORAL
    t_next = min_reduce(all_timesteps)    // O(log P)
    
end while
```

---

## ğŸ“ **ANÃLISIS DE COMPLEJIDAD TEÃ“RICA**

### **Complejidad Secuencial**
```
T_seq = T_steps Ã— (T_predict + T_force + T_correct)

Donde:
- T_predict = O(N)           // PredicciÃ³n para N partÃ­culas
- T_force = O(NÂ²)            // N partÃ­culas Ã— N interacciones
- T_correct = O(N)           // CorrecciÃ³n para N partÃ­culas
- T_steps = O(t_end/dt_avg)  // NÃºmero de pasos temporales

Por tanto: T_seq = O(T_steps Ã— NÂ²)
```

### **Complejidad Paralela (P procesadores)**
```
T_par = T_steps Ã— (T_predict_par + T_comm + T_force_par + T_correct_par + T_reduce)

Donde:
- T_predict_par = O(N/P)     // PredicciÃ³n distribuida
- T_comm = O(N + log P)      // All-gather de posiciones
- T_force_par = O(NÂ²/P)      // Fuerzas distribuidas
- T_correct_par = O(N/P)     // CorrecciÃ³n distribuida  
- T_reduce = O(log P)        // ReducciÃ³n de timesteps

Dominante: T_par = O(T_steps Ã— (NÂ²/P + N + log P))
```

### **Speedup TeÃ³rico**
```
S(P) = T_seq / T_par = (T_steps Ã— NÂ²) / (T_steps Ã— (NÂ²/P + N + log P))

S(P) = NÂ² / (NÂ²/P + N + log P)

Para N >> P >> log P:
S(P) â‰ˆ P Ã— NÂ² / (NÂ² + PÃ—N) = P / (1 + P/N)

Speedup mÃ¡ximo: S_max â‰ˆ N (limitado por comunicaciÃ³n)
```

### **Eficiencia TeÃ³rica**
```
E(P) = S(P) / P = 1 / (1 + P/N + PÃ—log P/NÂ²)

Para eficiencia alta: P << N
Eficiencia Ã³ptima: P â‰ˆ âˆšN
```

---

## ğŸ“Š **PREDICCIONES TEÃ“RICAS**

### **Para N = 5,120 partÃ­culas:**

| Procesos (P) | Speedup TeÃ³rico | Eficiencia TeÃ³rica | Tiempo Estimado |
|--------------|-----------------|-------------------|-----------------|
| 1            | 1.0             | 100%              | 1200s           |
| 2            | 1.95            | 97%               | 615s            |
| 4            | 3.81            | 95%               | 315s            |
| 8            | 7.27            | 91%               | 165s            |
| 16           | 13.1            | 82%               | 92s             |
| 32           | 22.4            | 70%               | 54s             |

### **Punto Ã“ptimo:**
```
P_optimal = âˆšN = âˆš5120 â‰ˆ 72 procesos
Eficiencia esperada: ~85%
```

---

## âš¡ **FACTORES LIMITANTES**

### **1. ComunicaciÃ³n (Overhead)**
```
T_comm = Î± + Î² Ã— N
Donde:
- Î± = latencia de red (~10â»â¶ s)
- Î² = ancho de banda inverso (~10â»â¹ s/byte)
- N Ã— 24 bytes por partÃ­cula (pos + vel)
```

### **2. Desbalance de Carga**
```
Timesteps adaptativos â†’ diferentes dt por partÃ­cula
PartÃ­culas centrales: dt_min â‰ˆ 10â»â´
PartÃ­culas externas: dt_max â‰ˆ 10â»Â²
Ratio: dt_max/dt_min â‰ˆ 100:1
```

### **3. SincronizaciÃ³n**
```
Barrera global cada paso temporal
T_sync = O(log P) por sincronizaciÃ³n
Impacto: ~5-10% del tiempo total
```

---

## ğŸ¯ **VALIDACIÃ“N EXPERIMENTAL**

### **MÃ©tricas a Medir:**
1. **Tiempo total** vs P (procesos)
2. **Tiempo de cÃ³mputo** vs P  
3. **Tiempo de comunicaciÃ³n** vs P
4. **Speedup real** vs teÃ³rico
5. **Eficiencia** vs P
6. **Escalabilidad fuerte** (N fijo, P variable)
7. **Escalabilidad dÃ©bil** (N/P fijo, P variable)

### **Experimentos Propuestos:**
```bash
# Escalabilidad fuerte (N=5120 fijo)
for P in 1 2 4 8 16 32; do
    mpirun -n $P ./cpu-4th < config.cfg
done

# Escalabilidad dÃ©bil (N/P=320 fijo)  
for P in 1 2 4 8 16; do
    N=$((P * 320))
    ./gen-plum $N > data_${N}.inp
    mpirun -n $P ./cpu-4th < config_${N}.cfg
done
```

---

## ğŸ“ˆ **NORMALIZACIÃ“N PARA COMPARACIÃ“N**

### **Tiempo Normalizado:**
```
T_norm(P) = T_measured(P) / T_measured(1)
T_theory(P) = (NÂ²/P + N + log P) / NÂ²
```

### **FLOPS TeÃ³ricos:**
```
FLOPS_per_interaction = 20 (distancia + fuerza)
FLOPS_per_step = NÂ² Ã— 20 + N Ã— 60 (Hermite)
FLOPS_total = FLOPS_per_step Ã— T_steps

Para N=5120, T_steps=1000:
FLOPS_total â‰ˆ 5.2Ã—10Â¹Â¹ operaciones
```

---

## ğŸ”¬ **CONCLUSIONES TEÃ“RICAS**

1. **Algoritmo es paralelizable** con eficiencia alta para P < âˆšN
2. **Bottleneck principal:** CÃ¡lculo de fuerzas O(NÂ²)  
3. **ComunicaciÃ³n escalable:** O(N + log P)
4. **Speedup mÃ¡ximo teÃ³rico:** ~72x para N=5120
5. **LimitaciÃ³n prÃ¡ctica:** Desbalance por timesteps adaptativos

**PredicciÃ³n:** Eficiencia >80% hasta P=16, degradaciÃ³n gradual despuÃ©s.