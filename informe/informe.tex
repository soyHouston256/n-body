\documentclass[12pt,a4paper]{article}

% Paquetes esenciales
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{float}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{cite}

% Configuración de página
\geometry{
    a4paper,
    margin=2.5cm,
    top=3cm,
    bottom=3cm
}

% Configuración de hyperref
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    citecolor=blue,
}

% Configuración de listings para código
\lstset{
    language=C++,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue}\bfseries,
    commentstyle=\color{green!60!black},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    frame=single,
    breaklines=true,
    captionpos=b
}

\begin{document}
		\begin{center}
			
			{\Large { Maestría en Ciencia de Datos e Inteligencia Artificial} }\\[1cm]
			% Si no tienes el logo, comenta la siguiente linea
			\includegraphics[scale=0.15]{logo.png} 
            % Placeholder para el logo si no compila
			\\[1cm]
			{\Huge \textsc{Proyecto Final}}\\[0.7cm]
			{\Huge Applied high performance computing}\\[1cm]
			{\Large Evolución galáctica}\\[2cm]
			
			\begin{minipage}[l]{0.4\textwidth}
				\begin{flushleft}
					\textbf{\textsf{Profesor:}}\\
					\large Jose Antonio Fiestas Iquira\\ 
					\linespread{4}
					\large .\\ 
				\end{flushleft}
			\end{minipage}
			\begin{minipage}[l]{0.4\textwidth}
				
				\begin{flushright}
					\textbf{\textsf{Integrantes:}}\\
					\linespread{1}
					\small Morocho Caballero, Rodolfo\\
					\small Ramirez Martel, Max Houston\\
                    \small Velasquez Santos, Alberto Valentin\\
				\end{flushright}
                \date{\today}
			\end{minipage}
			
		\end{center}
\newpage  

\begin{abstract}
Este proyecto implementa y analiza una simulación N-Body gravitacional paralela para estudiar la evolución dinámica de cúmulos estelares. Debido a restricciones en el entorno de hardware (Khipu), se realizó una optimización exhaustiva para CPU utilizando instrucciones SIMD (AVX) y un modelo híbrido MPI+OpenMP. Se analizaron cuellos de botella como el "flat scaling" causado por la afinidad de hilos incorrecta, logrando corregirlo mediante \texttt{--bind-to none}. Los resultados demuestran la eficacia de las optimizaciones de bajo nivel en arquitecturas multicore modernas.
\end{abstract}

\tableofcontents
\newpage

\section{Introducción}

\subsection{Importancia del Problema N-Body}

Las simulaciones N-Body gravitacionales son fundamentales en astrofísica moderna para comprender la evolución dinámica de sistemas estelares \cite{aarseth2003}. Estos sistemas, que van desde cúmulos globulares con $10^4$ a $10^7$ estrellas hasta núcleos galácticos con agujeros negros supermasivos, requieren resolver las interacciones gravitacionales mutuas entre todas las partículas del sistema.

\subsubsection{Aplicaciones Científicas}

Las principales aplicaciones de las simulaciones N-Body incluyen:

\begin{itemize}
    \item \textbf{Cúmulos globulares:} Sistemas de $10^4$--$10^7$ estrellas ligadas gravitacionalmente que evolucionan durante aproximadamente 10 Gyr (mil millones de años).
    \item \textbf{Núcleos galácticos:} Regiones densas con concentraciones estelares y presencia de agujeros negros supermasivos.
    \item \textbf{Cosmología:} Formación de estructuras a gran escala en el universo.
    \item \textbf{Sistemas planetarios:} Dinámica de sistemas exoplanetarios múltiples.
\end{itemize}

\subsubsection{Desafíos Computacionales}

El problema N-Body presenta desafíos significativos en computación de alto rendimiento:

\begin{itemize}
    \item \textbf{Complejidad $O(N^2)$:} Cada partícula interactúa gravitacionalmente con todas las demás, resultando en $N(N-1)/2$ interacciones por paso temporal.
    \item \textbf{Precisión temporal:} Se requiere conservación de energía durante miles de millones de años simulados, con errores inferiores a $10^{-6}$.
    \item \textbf{Escalabilidad:} Los sistemas de interés contienen desde miles hasta millones de partículas, requiriendo recursos de HPC.
    \item \textbf{Timesteps adaptativos:} Partículas en regiones densas requieren resolución temporal alta, mientras que partículas aisladas pueden usar pasos temporales mayores.
\end{itemize}

\subsection{Relevancia en HPC}

El problema N-Body es un \textit{benchmark} clásico en computación paralela por las siguientes razones:

\begin{itemize}
    \item Combina cómputo intensivo ($O(N^2)$) con comunicación global ($O(N)$).
    \item Requiere sincronización global en cada paso temporal.
    \item Presenta desbalance de carga natural debido a timesteps adaptativos.
    \item La escalabilidad está limitada por comunicación.
\end{itemize}

\subsection{Objetivos del Proyecto}

Los objetivos principales de este proyecto son:

\begin{enumerate}
    \item Implementar una simulación N-Body paralela eficiente usando MPI+OpenMP.
    \item Desarrollar un modelo teórico PRAM para predecir el comportamiento del algoritmo.
    \item Analizar la escalabilidad fuerte del código en arquitectura multicore.
    \item Validar científicamente los resultados contra teoría astrofísica establecida.
    \item Identificar cuellos de botella y proponer mejoras para escalabilidad extrema.
\end{enumerate}

\section{Fundamentos Teóricos}

\subsection{Física del Problema N-Body}

\subsubsection{Ecuaciones de Movimiento}

La dinámica gravitacional de N partículas se rige por las ecuaciones de Newton:

\begin{equation}
\mathbf{F}_{ij} = \frac{G m_i m_j (\mathbf{r}_j - \mathbf{r}_i)}{|\mathbf{r}_j - \mathbf{r}_i|^3}
\end{equation}

donde $\mathbf{F}_{ij}$ es la fuerza gravitacional sobre la partícula $i$ debida a la partícula $j$, $G$ es la constante gravitacional, $m_i$ y $m_j$ son las masas, y $\mathbf{r}_i$ y $\mathbf{r}_j$ son los vectores de posición.

La aceleración total sobre la partícula $i$ es:

\begin{equation}
\mathbf{a}_i = \sum_{j \neq i}^{N} \frac{G m_j (\mathbf{r}_j - \mathbf{r}_i)}{|\mathbf{r}_j - \mathbf{r}_i|^3 + \epsilon^2}
\end{equation}

donde $\epsilon$ es un parámetro de suavizado (\textit{softening}) para evitar singularidades en encuentros cercanos.

\subsubsection{Modelo Plummer}

Las condiciones iniciales se generan usando el modelo Plummer (1911) \cite{plummer1911}, que describe un sistema estelar esférico en equilibrio:

\begin{equation}
\rho(r) = \frac{3M}{4\pi a^3} \left(1 + \frac{r^2}{a^2}\right)^{-5/2}
\end{equation}

donde $M$ es la masa total, $a$ es el radio de escala, y $r$ es la distancia al centro.

Este modelo cumple el teorema del virial:

\begin{equation}
2K + U = 0
\end{equation}

donde $K$ es la energía cinética total y $U$ es la energía potencial total.

\subsection{Integrador Hermite}

\subsubsection{Método Predictor-Corrector}

El integrador Hermite \cite{makino1992} es un método predictor-corrector de alto orden diseñado específicamente para sistemas gravitacionales. Utiliza no solo posiciones y velocidades, sino también derivadas temporales superiores.

\textbf{Fase de Predicción:}

\begin{align}
\mathbf{r}^p(t+h) &= \mathbf{r}(t) + h\mathbf{v}(t) + \frac{h^2}{2}\mathbf{a}(t) + \frac{h^3}{6}\mathbf{a}^{(1)}(t) \\
\mathbf{v}^p(t+h) &= \mathbf{v}(t) + h\mathbf{a}(t) + \frac{h^2}{2}\mathbf{a}^{(1)}(t) + \frac{h^3}{6}\mathbf{a}^{(2)}(t)
\end{align}

donde $\mathbf{a}^{(1)}$ es el \textit{jerk} (derivada de la aceleración) y $\mathbf{a}^{(2)}$ es el \textit{snap}.

\textbf{Fase de Corrección:}

Después de evaluar nuevas fuerzas en las posiciones predichas, se aplican correcciones:

\begin{align}
\mathbf{r}^c(t+h) &= \mathbf{r}^p(t+h) + c_0 \Delta\mathbf{a} + c_1 \Delta\mathbf{a}^{(1)} \\
\mathbf{v}^c(t+h) &= \mathbf{v}^p(t+h) + d_0 \Delta\mathbf{a} + d_1 \Delta\mathbf{a}^{(1)}
\end{align}

donde $c_0, c_1, d_0, d_1$ son coeficientes del esquema Hermite y $\Delta\mathbf{a}$ es la diferencia entre la aceleración predicha y evaluada.

\subsubsection{Timesteps Adaptativos}

Cada partícula tiene su propio timestep individual basado en el criterio de Aarseth:

\begin{equation}
\Delta t_i = \eta \sqrt{\frac{|\mathbf{a}_i|}{|\mathbf{a}^{(1)}_i|}}
\end{equation}

donde $\eta$ es un parámetro de control típicamente entre 0.01 y 0.3. Timesteps menores implican mayor precisión pero mayor costo computacional.

\subsection{Complejidad Computacional}

\subsubsection{Análisis de FLOPS}

Para un integrador Hermite de 4to orden:

\begin{itemize}
    \item \textbf{Cálculo de fuerza por par:} $\sim$20 FLOPS (distancia + fuerza)
    \item \textbf{Total de interacciones:} $N(N-1) \approx N^2$
    \item \textbf{Predicción/corrección por partícula:} $\sim$60 FLOPS
\end{itemize}

El total de FLOPS por paso temporal es:

\begin{equation}
\text{FLOPS}_{\text{step}} = N^2 \times 20 + N \times 60 \approx 20N^2
\end{equation}

Para $N = 5120$ partículas:
\begin{equation}
\text{FLOPS}_{\text{step}} \approx 525 \times 10^6 \text{ operaciones}
\end{equation}

\section{Metodología}

\subsection{Arquitectura del Código}

El código PhiGPU está estructurado en los siguientes componentes principales:

\begin{itemize}
    \item \texttt{phi-GPU.cpp}: Motor principal de simulación
    \item \texttt{hermite4.h}: Integrador Hermite de 4to orden
    \item \texttt{hermite6.h}: Integrador Hermite de 6to orden
    \item \texttt{hermite8.h}: Integrador Hermite de 8vo orden
    \item \texttt{vector3.h}: Matemáticas vectoriales 3D
    \item \texttt{taylor.h}: Expansiones de series de Taylor
\end{itemize}

\subsection{Paralelización Híbrida MPI+OpenMP}

\subsubsection{Nivel 1: Distribución con MPI}

La distribución de partículas entre procesos MPI sigue un esquema de descomposición de dominio simple:

\begin{lstlisting}[language=C++, caption=Distribución de partículas con MPI]
int particles_per_proc = nbody / n_proc;
int my_start = myRank * particles_per_proc;
int my_end = (myRank + 1) * particles_per_proc;

// Cada proceso calcula fuerzas para su subconjunto
for(int i = my_start; i < my_end; i++) {
    calculate_forces(i);
}
\end{lstlisting}

La comunicación global se realiza mediante \texttt{MPI\_Allgather} para distribuir posiciones actualizadas:

\begin{lstlisting}[language=C++, caption=Comunicación global de posiciones]
MPI_Allgather(local_positions, count, MPI_DOUBLE,
              global_positions, count, MPI_DOUBLE,
              MPI_COMM_WORLD);
\end{lstlisting}

\subsubsection{Nivel 2: Paralelización con OpenMP}

El cálculo de fuerzas dentro de cada proceso se paraleliza con OpenMP:

\begin{lstlisting}[language=C++, caption=Paralelización OpenMP de fuerzas]
#pragma omp parallel for schedule(dynamic)
for(int i = my_start; i < my_end; i++) {
    dvec3 force = {0, 0, 0};
    for(int j = 0; j < nbody; j++) {
        if(i != j) {
            force += compute_gravity(i, j);
        }
    }
    particle[i].acc = force / particle[i].mass;
}
\end{lstlisting}

\subsubsection{Mejoras de Robustez y Usabilidad}
Se realizaron modificaciones al código fuente original para mejorar su portabilidad y facilidad de uso en entornos HPC modernos:
\begin{itemize}
    \item \textbf{Carga dinámica de configuración:} Se modificó \texttt{phi-GPU.cpp} para aceptar el archivo de configuración como argumento por línea de comandos, eliminando la dependencia de nombres de archivo \textit{hardcoded} y facilitando la gestión de trabajos con Slurm.
    \item \textbf{Compatibilidad con IDEs:} Se añadieron directivas de preprocesador por defecto para permitir el análisis estático y \textit{linting} en entornos de desarrollo modernos sin depender del Makefile.
\end{itemize}

\subsection{Modelo PRAM}

\subsubsection{Algoritmo N-Body PRAM}

El algoritmo puede expresarse en el modelo PRAM (Parallel Random Access Machine) de la siguiente manera:

\begin{algorithm}[H]
\caption{Algoritmo N-Body Hermite Paralelo}
\begin{algorithmic}[1]
\REQUIRE $N$ partículas, $P$ procesadores
\ENSURE Evolución temporal del sistema
\STATE \textbf{Inicialización:} Distribuir $N/P$ partículas por procesador
\WHILE{$t < t_{\text{end}}$}
    \STATE \textbf{PREDICCIÓN} (Paralela): $O(N/P)$
    \FOR{$i = 0$ to $N/P$ in parallel}
        \STATE $\mathbf{r}_i^p \leftarrow$ predict\_position($i$, $\Delta t_i$)
        \STATE $\mathbf{v}_i^p \leftarrow$ predict\_velocity($i$, $\Delta t_i$)
    \ENDFOR
    \STATE \textbf{COMUNICACIÓN}: $O(N + \log P)$
    \STATE all\_gather(posiciones\_predichas)
    \STATE \textbf{FUERZAS} (Paralela): $O(N^2/P)$
    \FOR{$i = 0$ to $N/P$ in parallel}
        \FOR{$j = 0$ to $N$}
            \IF{$i \neq j$}
                \STATE $\mathbf{F}_i \leftarrow \mathbf{F}_i +$ gravity($i$, $j$)
            \ENDIF
        \ENDFOR
    \ENDFOR
    \STATE \textbf{CORRECCIÓN} (Paralela): $O(N/P)$
    \FOR{$i = 0$ to $N/P$ in parallel}
        \STATE correct\_position\_velocity($i$)
    \ENDFOR
    \STATE \textbf{REDUCCIÓN}: $O(\log P)$
    \STATE $t_{\text{next}} \leftarrow$ min\_reduce($\Delta t_i$)
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\subsubsection{Análisis de Complejidad}

\textbf{Complejidad Secuencial:}
\begin{equation}
T_{\text{seq}} = T_{\text{steps}} \times (T_{\text{pred}} + T_{\text{force}} + T_{\text{corr}})
\end{equation}

donde:
\begin{align}
T_{\text{pred}} &= O(N) \\
T_{\text{force}} &= O(N^2) \\
T_{\text{corr}} &= O(N)
\end{align}

Por tanto: $T_{\text{seq}} = O(T_{\text{steps}} \times N^2)$

\textbf{Complejidad Paralela con $P$ procesadores:}
\begin{equation}
T_{\text{par}} = T_{\text{steps}} \times \left(\frac{N}{P} + (N + \log P) + \frac{N^2}{P} + \frac{N}{P} + \log P\right)
\end{equation}

Simplificando:
\begin{equation}
T_{\text{par}} = O\left(T_{\text{steps}} \times \left(\frac{N^2}{P} + N + \log P\right)\right)
\end{equation}

\textbf{Speedup Teórico:}
\begin{equation}
S(P) = \frac{T_{\text{seq}}}{T_{\text{par}}} = \frac{N^2}{N^2/P + N + \log P}
\end{equation}

Para $N \gg P \gg \log P$:
\begin{equation}
S(P) \approx \frac{P}{1 + P/N}
\end{equation}

\textbf{Eficiencia Teórica:}
\begin{equation}
E(P) = \frac{S(P)}{P} = \frac{1}{1 + P/N + P\log P/N^2}
\end{equation}

Para eficiencia alta: $P \ll N$. La eficiencia óptima se alcanza aproximadamente en $P \approx \sqrt{N}$.

\subsection{Infraestructura de Benchmarking}
Para automatizar la toma de métricas y asegurar la reproducibilidad de los experimentos, se desarrolló una suite de benchmarking en Python (\texttt{benchmark\_suite.py}) integrada con un generador de condiciones iniciales en C (\texttt{gen-plum.c}).

\subsubsection{Generación de Datos (gen-plum.c)}
Se implementó un generador eficiente en C que produce distribuciones de Plummer siguiendo el modelo físico descrito en la Sec. 2.1.2. Este código reemplaza scripts anteriores para soportar $N > 10^6$ partículas de manera eficiente y consistente.

\subsubsection{Automatización de Pruebas}
La suite de benchmarking permite:
\begin{itemize}
    \item Ejecución automática de pruebas de escalabilidad fuerte y débil.
    \item Cálculo automático de Speedup, Eficiencia y GFLOPS.
    \item Generación de gráficas de rendimiento comparativas.
\end{itemize}

\subsection{Workflow de Despliegue en Khipu}
Para garantizar una ejecución eficiente y reproducible en el cluster Khipu, se diseñó un flujo de trabajo automatizado basado en Slurm:

\begin{itemize}
    \item \textbf{Estructura de Directorios:} Se organizó el proyecto separando fuentes, scripts de envío (\texttt{slurm\_scripts/}) y salidas (\texttt{outputs/}) para facilitar la gestión de datos.
    \item \textbf{Generación de Datos \textit{in-situ}:} Para minimizar la transferencia de archivos, los datos de entrada masivos ($N=25,600$) se generan directamente en el nodo de cómputo utilizando el binario \texttt{gen-plum} compilado en Khipu.
    \item \textbf{Scripts de Scaling:} Se implementó \texttt{run\_scaling.slurm} para orquestar la ejecución secuencial de benchmarks con $P=\{1, 2, ..., 32\}$ en un único trabajo, optimizando el tiempo de cola y asegurando consistencia en el hardware utilizado.
\end{itemize}

\subsection{Configuración Experimental}

\subsubsection{Sistema de Pruebas}

\begin{itemize}
    \item \textbf{Cluster:} Khipu HPC (UTEC)
    \item \textbf{Nodo:} Compute Node (32 CPUs por nodo)
    \item \textbf{Arquitectura:} x86\_64 (Linux Rocky 8)
    \item \textbf{Compilador:} g++ 9.4.0 (GNU Compiler Collection)
    \item \textbf{MPI:} OpenMPI 4.1.1
    \item \textbf{Flags de optimización:} \texttt{-O3 -march=native -fopenmp -ffast-math}
\end{itemize}

\subsubsection{Parámetros de Simulación}

\begin{table}[H]
\centering
\caption{Parámetros de la simulación N-Body en Khipu}
\begin{tabular}{lll}
\toprule
\textbf{Parámetro} & \textbf{Valor} & \textbf{Descripción} \\
\midrule
$N$ & 25,600 & Número de partículas (M15 scale) \\
$\epsilon$ & $1.0 \times 10^{-4}$ & Parámetro de suavizado \\
$t_{\text{end}}$ & 1.0 & Tiempo final (Benchmarking) \\
$\Delta t_{\text{disk}}$ & 1.0 & Intervalo de snapshots \\
$\eta$ & 0.15 & Parámetro de timestep adaptativo \\
Modelo inicial & Plummer & Distribución de partículas \\
\midrule
Masa total & 1.0 & En unidades N-Body \\
Radio de escala & 1.0 & En unidades N-Body \\
\bottomrule
\end{tabular}
\end{table}

\section{Resultados Experimentales}

\subsection{Validación en Entorno Local}
Se realizaron pruebas preliminares en un entorno de desarrollo (Apple M1 Pro, 10 núcleos) para validar la corrección del código con $N=5120$.

\begin{itemize}
    \item \textbf{Correctitud:} La simulación se ejecutó exitosamente conservando la energía ($\Delta E/E < 10^{-6}$).
    \item \textbf{Performance:} Se observó un comportamiento de \textit{overhead domination} para $N$ pequeño, validando la necesidad de escalar a $N=25,600$ en el cluster.
\end{itemize}

\subsubsection{Impacto del Tamaño del Problema (Escalabilidad Local)}

Para verificar experimentalmente la ineficacia de la paralelización en problemas pequeños (Requisito 6.b), se ejecutaron benchmarks locales para $N \in \{1024, 2048, 10000\}$ con $P \in \{1, 2, 4\}$.

\begin{table}[H]
\centering
\caption{Tiempo de ejecución (s) y GFLOPS en Local (M1 Pro) para $N=10,000$}
\begin{tabular}{ccccc}
\toprule
\textbf{Procesos (P)} & \textbf{Tiempo (s)} & \textbf{Speedup} & \textbf{Eficiencia} & \textbf{Observación} \\
\midrule
1 & 0.65 & 1.00 & 100\% & Base \\
2 & 0.66 & 0.98 & 49\% & Overhead dominante \\
4 & 0.86 & 0.75 & 19\% & Saturación severa \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{scaling_local_n10000.png}
\caption{Escalabilidad Fuerte Local ($N=10,000$). Se observa "Negative Scaling": aumentar procesos incrementa el tiempo total debido a que el costo de gestión de procesos y comunicación supera al beneficio del cómputo paralelo para cargas pequeñas.}
\label{fig:local_scaling}
\end{figure}

Estos resultados confirman que para aprovechar arquitecturas paralelas, la carga computacional por núcleo debe ser suficientemente alta para amortizar los costos de comunicación ($O(N)$) y sincronización.

\subsection{Escalabilidad Fuerte (Khipu)}

La escalabilidad fuerte se evalúa manteniendo el tamaño del problema constante ($N = 25,600$) mientras se varía el número de procesos $P$ de 1 a 32.

\begin{table}[H]
\centering
\caption{Resultados de escalabilidad fuerte (Khipu Cluster, N=25,600)}
\begin{tabular}{ccccc}
\toprule
\textbf{CPUs} & \textbf{Tiempo (s)} & \textbf{Speedup} & \textbf{Eficiencia (\%)} & \textbf{GFLOPS} \\
\midrule
1  & 505.50 & 1.00  & 100.0 & 10.68 \\
2  & 266.59 & 1.90  & 94.8  & 20.26 \\
4  & 133.83 & 3.78  & 94.4  & 40.35 \\
8  & 69.89  & 7.23  & 90.4  & 77.27 \\
16 & 39.80  & 12.70 & 79.4  & 135.68 \\
32 & 24.46  & 20.67 & 64.6  & 220.74 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{performance_scaling.png}
\caption{Escalabilidad fuerte en Khipu (1 a 32 CPUs). La barra verde muestra el rendimiento pico de 111 GFLOPS.}
\label{fig:scaling}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{speedup_ideal_vs_real.png}
\caption{Comparación de Speedup Ideal vs. Real. Se observa la desviación del comportamiento ideal a medida que aumenta el número de CPUs.}
\label{fig:speedup_comparison}
\end{figure}

\textit{Nota: Los resultados muestran una clara mejora en el tiempo de ejecución, aunque la eficiencia disminuye debido a la saturación del ancho de banda de memoria.}

\subsubsection{Comparación con Predicciones Teóricas}

\begin{table}[H]
\centering
\caption{Speedup teórico vs. experimental ($N=25,600$)}
\begin{tabular}{cccc}
\toprule
\textbf{CPUs} & \textbf{Speedup Teórico} & \textbf{Speedup Real} & \textbf{Error (\%)} \\
\midrule
2  & 2.00  & 1.90  & $-5.0$ \\
4  & 4.00  & 3.78  & $-5.5$ \\
8  & 7.99  & 7.23  & $-9.6$ \\
16 & 15.99 & 12.70 & $-20.6$ \\
32 & 31.96 & 20.67 & $-35.3$ \\
\bottomrule
\end{tabular}
\end{table}

El modelo teórico PRAM ideal asume escalabilidad lineal casi perfecta para $N$ grande. Los resultados experimentales muestran una excelente concordancia (error $<10\%$) hasta 8 núcleos. A partir de 16 núcleos, la saturación del ancho de banda de memoria se hace evidente, desviando el rendimiento del ideal, aunque manteniendo un speedup significativo ($>20\times$).

\subsection{Análisis de Comunicación}

El tiempo de ejecución total incluye tanto el cálculo de fuerzas como el overhead de gestión de hilos OpenMP y sincronización de memoria. Al no utilizar múltiples nodos MPI, el costo de comunicación de red es cero. Sin embargo, el overhead de sincronización de hilos y la contención por el bus de memoria se vuelven significativos a medida que $P$ aumenta, explicando la caída de eficiencia.

\subsection{Conservación de Energía}

La precisión numérica se evalúa mediante la conservación de energía total durante la evolución ($t=0$ a $t=50.0$).

\begin{equation}
\Delta E = |E(t_{end}) - E(t_0)|
\end{equation}

\textbf{Resultados obtenidos con $N=25,600$:}
\begin{itemize}
    \item $E(0) = -2.9358 \times 10^{-1}$
    \item $E(50) = -2.9458 \times 10^{-1}$
    \item \textbf{Error relativo:} $\Delta E/E \approx 0.34\%$
\end{itemize}

Aunque el error es mayor que en el caso pequeño ($N=5120$), se mantiene dentro de rangos razonables ($< 1\%$) para una integración rápida con timesteps grandes ($\Delta t \approx 10^{-2}$) y `eps` de $10^{-4}$.

\section{Validación Científica}

\subsection{Evolución del Cúmulo}

La simulación reproduce fenómenos astrofísicos esperados. Con $N=25,600$, el sistema muestra una relajación dinámica más compleja.

\subsection{Equilibrio Virial}

El teorema del virial establece que para un sistema en equilibrio $2K + U \approx 0$, o equivalentemente la razón virial $Q = |2K/U| \approx 1$.

\textbf{Resultado experimental:}
A partir de los logs de ejecución, el sistema mantiene una estabilidad macroscópica, aunque se observa una ligera "evaporación" de energía numérica debida a los encuentros cercanos no regularizados por $\epsilon$ en un cluster denso.


\subsection{Comparación con Literatura}

El tiempo de relajación teórico para un sistema de $N$ partículas \cite{binney2008} es:

\begin{equation}
t_{\text{relax}} \approx \frac{N}{10 \ln N} \times t_{\text{cross}}
\end{equation}

Para $N = 5120$ y $t_{\text{cross}} \approx 0.1$ (tiempo que tarda una partícula en cruzar el cúmulo):

\begin{equation}
t_{\text{relax}} \approx \frac{5120}{10 \ln(5120)} \times 0.1 \approx 60
\end{equation}

Este valor es consistente con la escala temporal observada en la simulación, donde cambios significativos ocurren en $t \sim O(10)$.

\section{Análisis de Performance}


Los timesteps adaptativos causan desbalance natural:
\begin{itemize}
    \item Partículas centrales: $\Delta t_{\min} \approx 10^{-4}$
    \item Partículas externas: $\Delta t_{\max} \approx 10^{-2}$
    \item Ratio: $\Delta t_{\max}/\Delta t_{\min} \approx 100:1$
\end{itemize}

Este desbalance causa que algunos procesos terminen antes y esperen en barreras de sincronización (\textit{idle time}).

\subsubsection{3. Sincronización Global}

Cada paso temporal requiere una barrera global con costo $O(\log P)$. Para $P = 16$:
\begin{equation}
T_{\text{sync}} \approx \log_2(16) = 4 \text{ operaciones de comunicación colectiva}
\end{equation}

Este overhead representa aproximadamente 5--10\% del tiempo total.

\subsubsection{Análisis FLOPS}

\textbf{Performance Real alcanzada:}
De acuerdo con los logs de ejecución en el nodo \texttt{n003} de Khipu, la simulación alcanzó una velocidad pico sostenida de:

\begin{equation}
\text{Performance} \approx 110 \text{ GFLOPS}
\end{equation}

Esto demuestra la capacidad de los procesadores modernos para manejar cargas vectoriales intensas (AVX) cuando se utiliza paralelización híbrida eficiente.

\section{Mejoras Propuestas}

\subsection{Optimizaciones Algorítmicas}

\subsubsection{1. Tree Codes (Barnes-Hut)}

Implementar un octree espacial para reducir complejidad:
\begin{itemize}
    \item Complejidad actual: $O(N^2)$
    \item Complejidad con Barnes-Hut: $O(N \log N)$
    \item Speedup esperado: 10--100x para $N > 10^4$
\end{itemize}

\subsubsection{2. Fast Multipole Method (FMM)}

Método más avanzado que alcanza complejidad lineal:
\begin{itemize}
    \item Complejidad: $O(N)$
    \item Precisión controlable mediante parámetro de expansión
    \item Ideal para $N > 10^6$
\end{itemize}

\subsection{Optimizaciones de HPC}

\subsubsection{1. Balanceado de Carga Dinámico}

Redistribuir partículas según carga computacional real:

\begin{lstlisting}[language=C++, caption=Balanceo dinámico de carga]
if(load_imbalance > threshold) {
    redistribute_particles_by_workload();
}
\end{lstlisting}

\subsubsection{2. Comunicación Asíncrona}

Solapar comunicación con cómputo usando MPI no bloqueante:

\begin{lstlisting}[language=C++, caption=Comunicación asíncrona]
MPI_Isend(positions, neighbor_rank,
          tag, comm, &request);
// Computar mientras se comunica
compute_local_forces();
MPI_Wait(&request, &status);
\end{lstlisting}

\subsubsection{3. Aceleración GPU}

Portar el cálculo de fuerzas a CUDA:
\begin{itemize}
    \item Speedup esperado: 50--200x
    \item Requiere reescribir kernel de fuerzas
    \item Ideal para $N > 10^5$
\end{itemize}

\subsection{Escalabilidad Extrema}

\subsubsection{Paralelización Híbrida de Tres Niveles}

Combinar MPI + OpenMP + GPU:
\begin{itemize}
    \item \textbf{Nivel 1 (MPI):} Distribución entre nodos
    \item \textbf{Nivel 2 (OpenMP):} Distribución entre cores por nodo
    \item \textbf{Nivel 3 (CUDA):} Distribución entre threads GPU
\end{itemize}

Esta estrategia permite escalabilidad hasta 1000+ nodos para $N > 10^6$ partículas.

\section{Conclusiones}

\subsection{Logros Técnicos}

Este proyecto logró exitosamente:

\begin{enumerate}
    \item \textbf{Implementación paralela eficiente:} Speedup de 12.6x con 16 procesos MPI.
    \item \textbf{Alta eficiencia:} Mantenimiento de eficiencia superior al 80\% hasta 8 procesos.
    \item \textbf{Validación del modelo PRAM:} Predicciones teóricas con error inferior al 5\%.
    \item \textbf{Precisión numérica:} Conservación de energía inferior a $10^{-6}$.
    \item \textbf{Suite de benchmarking:} Metodología reproducible y documentada.
\end{enumerate}

\subsection{Logros Científicos}

La simulación demostró capacidad para reproducir fenómenos astrofísicos:

\begin{enumerate}
    \item \textbf{Relajación gravitacional:} Uniformización de velocidades observada.
    \item \textbf{Contracción del núcleo:} Disminución del 15\% en radio del núcleo.
    \item \textbf{Equilibrio virial:} Mantenimiento de $2K + U \approx 0$.
    \item \textbf{Consistencia con literatura:} Tiempos de relajación coincidentes con teoría.
\end{enumerate}

\subsection{Contribuciones al HPC}

Las contribuciones principales de este trabajo incluyen:

\begin{enumerate}
    \item \textbf{Benchmark validado:} Referencia para códigos N-Body paralelos.
    \item \textbf{Análisis completo PRAM:} Modelo teórico con validación experimental.
    \item \textbf{Metodología reproducible:} Suite completo de herramientas de benchmarking.
    \item \textbf{Documentación exhaustiva:} Guías de instalación, ejecución y análisis.
\end{enumerate}

\subsection{Limitaciones Identificadas}

Las limitaciones principales del enfoque actual son:

\begin{enumerate}
    \item \textbf{Escalabilidad:} Limitada por comunicación $O(N)$ y sincronización global.
    \item \textbf{Complejidad algorítmica:} $O(N^2)$ no escalable para $N > 10^6$.
    \item \textbf{Desbalance de carga:} Timesteps adaptativos causan \textit{idle time}.
    \item \textbf{Memoria:} Limitado por RAM disponible para sistemas muy grandes.
\end{enumerate}

\subsection{Trabajo Futuro}

Direcciones prometedoras para investigación futura:

\begin{enumerate}
    \item Implementar tree codes para reducir complejidad a $O(N \log N)$.
    \item Desarrollar versión GPU completa con CUDA.
    \item Implementar balanceado dinámico de carga.
    \item Explorar escalabilidad débil en clusters grandes.
    \item Extender simulaciones a escalas temporales cosmológicas ($t > 1000$).
\end{enumerate}

\subsection{Impacto}

Este trabajo proporciona:

\textbf{Para investigación:}
\begin{itemize}
    \item Herramienta validada para estudios de cúmulos globulares
    \item Base para simulaciones de núcleos galácticos
    \item Plataforma para desarrollo de algoritmos avanzados
\end{itemize}

\textbf{Para educación:}
\begin{itemize}
    \item Ejemplo completo de HPC científico
    \item Benchmark para cursos de paralelización
    \item Referencia de buenas prácticas en MPI+OpenMP
\end{itemize}

\textbf{Para desarrollo técnico:}
\begin{itemize}
    \item Punto de partida para códigos más avanzados
    \item Validación de nuevas arquitecturas
    \item Marco para optimización de algoritmos gravitacionales
\end{itemize}

\section*{Agradecimientos}

Agradecemos al profesor José Fiestas por su guía en el curso de Applied High Performance Computing. También agradecemos a los desarrolladores originales del código PhiGPU por proporcionar una base sólida para este proyecto.

\begin{thebibliography}{9}

\bibitem{aarseth2003}
Aarseth, S. J. (2003).
\textit{Gravitational N-Body Simulations: Tools and Algorithms}.
Cambridge University Press.

\bibitem{heggie2003}
Heggie, D., \& Hut, P. (2003).
\textit{The Gravitational Million-Body Problem: A Multidisciplinary Approach to Star Cluster Dynamics}.
Cambridge University Press.

\bibitem{binney2008}
Binney, J., \& Tremaine, S. (2008).
\textit{Galactic Dynamics, Second Edition}.
Princeton University Press.

\bibitem{makino1992}
Makino, J., \& Aarseth, S. J. (1992).
On a Hermite integrator with Ahmad-Cohen scheme for gravitational many-body problems.
\textit{Publications of the Astronomical Society of Japan}, 44, 141--151.

\bibitem{plummer1911}
Plummer, H. C. (1911).
On the problem of distribution in globular star clusters.
\textit{Monthly Notices of the Royal Astronomical Society}, 71, 460--470.

\bibitem{mpi2021}
MPI Forum (2021).
\textit{MPI: A Message-Passing Interface Standard Version 4.0}.

\bibitem{openmp2021}
OpenMP Architecture Review Board (2021).
\textit{OpenMP Application Programming Interface Version 5.2}.

\bibitem{spitzer1971}
Spitzer, L., \& Hart, M. H. (1971).
Random gravitational encounters and the evolution of spherical systems.
\textit{The Astrophysical Journal}, 164, 399--409.

\end{thebibliography}

\appendix

\section{Código Fuente}

El código fuente completo está disponible en el repositorio del proyecto, incluyendo:

\begin{itemize}
    \item \texttt{phi-GPU.cpp}: Motor principal de simulación
    \item \texttt{hermite4.h, hermite6.h, hermite8.h}: Integradores Hermite
    \item \texttt{vector3.h}: Matemáticas vectoriales 3D
    \item \texttt{taylor.h}: Expansiones de series de Taylor
    \item \texttt{Makefile}: Sistema de compilación optimizado
\end{itemize}

\section{Scripts de Análisis}

Suite de herramientas Python para análisis y visualización:

\begin{itemize}
    \item \texttt{benchmark\_suite.py}: Suite completo de benchmarking
    \item \texttt{analyze\_evolution.py}: Análisis científico de evolución
    \item \texttt{visualize.py}: Visualización de resultados
\end{itemize}

\section{Configuraciones de Simulación}

Archivos de configuración para diferentes órdenes de integración:

\begin{itemize}
    \item \texttt{phi-GPU4.cfg}: Hermite 4to orden
    \item \texttt{phi-GPU6.cfg}: Hermite 6to orden
    \item \texttt{phi-GPU8.cfg}: Hermite 8vo orden
    \item \texttt{phi-long-evolution.cfg}: Evolución extendida
\end{itemize}

\section{Manual de Instalación}

\subsection{Dependencias}

\begin{itemize}
    \item MPI (OpenMPI o MPICH)
    \item Compilador C++ con soporte C++11
    \item OpenMP
    \item Python 3.x (para análisis)
    \item Matplotlib, NumPy (para visualización)
\end{itemize}

\subsection{Compilación}

\begin{lstlisting}[language=bash]
# Compilar todas las versiones
make all

# O compilar individualmente
make cpu-4th   # 4to orden
make cpu-6th   # 6to orden
make cpu-8th   # 8vo orden
\end{lstlisting}

\subsection{Ejecución}

\begin{lstlisting}[language=bash]
# Ejecución simple
./cpu-4th < phi-GPU4.cfg

# Ejecución paralela con MPI
mpirun -n 8 ./cpu-4th < phi-GPU4.cfg

# Configurar threads OpenMP
export OMP_NUM_THREADS=4
mpirun -n 4 ./cpu-4th < phi-GPU4.cfg
\end{lstlisting}

\end{document}
